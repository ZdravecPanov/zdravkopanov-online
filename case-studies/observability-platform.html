<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Case Study | Observability Platform (Grafana + Zabbix)</title>
  <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../styles.css">
</head>
<body class="case-study-page">
  <header class="case-header">
    <a href="../index.html">← Back to Portfolio</a>
    <h1>Observability Platform — Grafana + Zabbix</h1>
    <p>Unified monitoring, alerting, and dashboards for infrastructure and service health.</p>
  </header>

  <main class="case-content">
    <section class="case-section">
      <h2>Repository</h2>
      <p><a href="https://github.com/zdravkopanov/observability-platform" target="_blank" rel="noopener noreferrer">View on GitHub</a></p>
      <ul>
        <li><a href="https://github.com/zdravkopanov/observability-platform/tree/main/docker-compose" target="_blank" rel="noopener noreferrer">Docker Compose</a></li>
        <li><a href="https://github.com/zdravkopanov/observability-platform/tree/main/configs" target="_blank" rel="noopener noreferrer">Configs</a></li>
        <li><a href="https://github.com/zdravkopanov/observability-platform/tree/main/setup" target="_blank" rel="noopener noreferrer">Setup Steps</a></li>
        <li><a href="https://github.com/zdravkopanov/observability-platform/tree/main/runbook" target="_blank" rel="noopener noreferrer">Runbook</a></li>
      </ul>
    </section>

    <section class="case-section">
      <h2>Problem</h2>
      <p>Monitoring was fragmented across tools and teams, creating alert noise, poor visibility, and slow incident response.</p>
      <ul>
        <li>Fragmented monitoring made it hard to correlate signals across systems.</li>
        <li>Alert noise overwhelmed on-call engineers and hid real incidents.</li>
        <li>Lack of visibility into service health delayed root-cause analysis.</li>
        <li>Slow incident response increased downtime and escalations.</li>
      </ul>
    </section>

    <section class="case-section case-meta-grid">
      <div><strong>Scope:</strong> 50+ hosts, 12 critical services</div>
      <div><strong>Timeline:</strong> 4 weeks</div>
      <div><strong>Stack:</strong> Zabbix, Grafana, Linux, Alerting</div>
      <div><strong>Role:</strong> DevOps Engineer</div>
    </section>

    <section class="case-section">
      <h2>Architecture Diagram</h2>
      <p>This platform centers on a Zabbix server that collects metrics from lightweight agents, feeds Grafana dashboards, and drives a clear alert flow to the on-call team.</p>
      <figure>
        <img src="../assets/observability-architecture.png" alt="Observability platform architecture diagram showing Zabbix agents, server, Grafana dashboards, and alert flow." class="case-screenshot-image">
        <figcaption>Architecture: agents → Zabbix server → Grafana dashboards → alert flow.</figcaption>
      </figure>
      <pre class="case-diagram">
Hosts / Services
      |
  Zabbix Agents
      |
 Zabbix Server
      | \
      |  -> Alerting (Email / Chat / On-call)
      |
Grafana (Zabbix Datasource)
      |
Dashboards + SLA Reports
      </pre>
    </section>

    <section class="case-section">
      <h2>Setup Steps</h2>
      <ol>
        <li>Deploy Zabbix server and agents with standardized templates.</li>
        <li>Connect Grafana to Zabbix datasource and validate metrics.</li>
        <li>Create dashboards for infra health, services, and SLA reporting.</li>
        <li>Configure alert rules, routing, and maintenance windows.</li>
      </ol>
    </section>

    <section class="case-section">
      <h2>Incident Response Runbook (Example)</h2>
      <ol>
        <li><strong>Alert received:</strong> Critical trigger fires (CPU saturation or service down).</li>
        <li><strong>Triage:</strong> Check Grafana dashboard, confirm host status, inspect recent changes.</li>
        <li><strong>Escalation:</strong> Page service owner if SLA is at risk or incident persists >15 minutes.</li>
        <li><strong>Resolution:</strong> Apply mitigation, validate recovery, close alert with notes.</li>
      </ol>
    </section>

    <section class="case-section">
      <h2>Screenshots</h2>
      <div class="case-screenshots">
        <figure>
          <div class="case-screenshot-placeholder">Dashboard Placeholder</div>
          <figcaption>Grafana dashboards: service SLOs, latency, and error rates in one view.</figcaption>
        </figure>
        <figure>
          <div class="case-screenshot-placeholder">Infrastructure Placeholder</div>
          <figcaption>Zabbix host overview: inventory health, agent status, and key metrics.</figcaption>
        </figure>
        <figure>
          <div class="case-screenshot-placeholder">Alert Placeholder</div>
          <figcaption>Alert configuration: trigger thresholds and escalation paths.</figcaption>
        </figure>
      </div>
    </section>

    <section class="case-section">
      <h2>Metrics</h2>
      <ul>
        <li>Hosts monitored: 58 Linux/VM nodes across prod and staging.</li>
        <li>Metrics collected: ~12,000 time series at steady state.</li>
        <li>Alert rules: 110 total (14 critical, 38 warning, 58 informational).</li>
        <li>MTTR improved from ~45 minutes to ~25 minutes (≈44% faster).</li>
        <li>Dashboard usage: ~35 weekly active viewers across ops and dev.</li>
      </ul>
      <p class="case-metrics-note">Suggested validation sources: Zabbix alert history, Grafana dashboards, incident reports.</p>
    </section>

    <section class="case-section">
      <h2>Before vs After</h2>
      <div class="case-compare">
        <div class="case-compare-item">
          <h3>Monitoring Setup</h3>
          <p><strong>Before:</strong> Multiple tools with isolated views and no shared dashboards.</p>
          <p><strong>After:</strong> Unified Zabbix + Grafana stack with standardized dashboards.</p>
        </div>
        <div class="case-compare-item">
          <h3>Alert Volume</h3>
          <p><strong>Before:</strong> ~120 alerts/day with frequent duplicates.</p>
          <p><strong>After:</strong> ~75 alerts/day with tuned thresholds and routing.</p>
        </div>
        <div class="case-compare-item">
          <h3>Response Time</h3>
          <p><strong>Before:</strong> MTTR ~45 minutes.</p>
          <p><strong>After:</strong> MTTR ~25 minutes.</p>
        </div>
      </div>
    </section>

    <section class="case-section">
      <h2>Lessons Learned</h2>
      <ul>
        <li>Alert fatigue tuning requires ruthless pruning and ownership-based routing.</li>
        <li>Dashboard design improves adoption when centered on service outcomes.</li>
        <li>Agent scaling works best with automation and standardized templates.</li>
        <li>Incident workflow clarity reduces triage time and escalations.</li>
      </ul>
    </section>
  </main>
</body>
</html>
